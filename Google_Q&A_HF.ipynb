{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nandanpr/ai4/blob/main/Google_Q%26A_HF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ct0WzsTeCj0",
        "outputId": "cdf267b4-6f8a-4a0f-e40d-47a0f77feb5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n",
            "Python 3.9.16\n",
            "There are 2 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.9   2         auto mode\n",
            "  1            /usr/bin/python3.8   1         manual mode\n",
            "  2            /usr/bin/python3.9   2         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: ^C\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!python3 --version\n",
        "\n",
        "!sudo update-alternatives --config python3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yg7ZC-qIwmY",
        "outputId": "1dc1c8a2-8125-4add-863e-410786e92a82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "update-alternatives: --install needs <link> <name> <path> <priority>\n",
            "\n",
            "Use 'update-alternatives --help' for program usage information.\n"
          ]
        }
      ],
      "source": [
        "!sudo update-alternatives --install /usr/bin/python3 python3 /Users/prasanthnandanuru/miniconda3/bin/python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXFwVAdmL-1l",
        "outputId": "84a495d1-b056-4229-aa1b-ebd9479a20a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to cloud.r-project.org] [Conn\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to ppa.launchpad.net] [Waitin\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to ppa.launchp\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r                                                                               \r0% [Waiting for headers] [Connecting to ppa.launchpad.net (185.125.190.52)]\r0% [Waiting for headers] [Connecting to ppa.launchpad.net (185.125.190.52)]\r                                                                           \rHit:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "\r0% [Waiting for headers] [Connecting to ppa.launchpad.net (185.125.190.52)]\r                                                                           \rHit:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package python3.10.10\n",
            "E: Couldn't find any package by glob 'python3.10.10'\n",
            "E: Couldn't find any package by regex 'python3.10.10'\n",
            "update-alternatives: --install needs <link> <name> <path> <priority>\n",
            "\n",
            "Use 'update-alternatives --help' for program usage information.\n",
            "update-alternatives: --install needs <link> <name> <path> <priority>\n",
            "\n",
            "Use 'update-alternatives --help' for program usage information.\n",
            "update-alternatives: error: no alternatives for python\n",
            "update-alternatives: error: no alternatives for python\n",
            "update-alternatives: error: no alternatives for python\n",
            "Python 3.9.16\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.10.10\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10\n",
        "\n",
        "!update-alternatives --install /usr/bin/python python /usr/bin/python3.10\n",
        "!update-alternatives --list python\n",
        "!sudo update-alternatives --config python\n",
        "!sudo update-alternatives --set python /usr/bin/python3.10\n",
        "!python3 --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z-kvNIleLQs",
        "outputId": "40cf583c-d4fd-46e2-c8cb-6ae17738cb94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.9/dist-packages (0.0.142)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.29.1)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: gptcache>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.1.23)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.4.48)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain) (0.27.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain) (5.3.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: llama-index==0.6.0.alpha3 in /usr/local/lib/python3.9/dist-packages (0.6.0a3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (0.4.0)\n",
            "Requirement already satisfied: langchain==0.0.142 in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (0.0.142)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (1.5.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (0.5.7)\n",
            "Requirement already satisfied: openai>=0.26.4 in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (0.27.6)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (8.2.2)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (2.8.4)\n",
            "Requirement already satisfied: gptcache>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (0.1.23)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (1.4.48)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (1.2.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (2.27.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (4.0.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (6.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (3.8.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (1.10.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->llama-index==0.6.0.alpha3) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->llama-index==0.6.0.alpha3) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->llama-index==0.6.0.alpha3) (0.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai>=0.26.4->llama-index==0.6.0.alpha3) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->llama-index==0.6.0.alpha3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->llama-index==0.6.0.alpha3) (2022.7.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken->llama-index==0.6.0.alpha3) (2022.10.31)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (1.9.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (23.1.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain==0.0.142->llama-index==0.6.0.alpha3) (5.3.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama-index==0.6.0.alpha3) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain==0.0.142->llama-index==0.6.0.alpha3) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index==0.6.0.alpha3) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.142->llama-index==0.6.0.alpha3) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.142->llama-index==0.6.0.alpha3) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.142->llama-index==0.6.0.alpha3) (2022.12.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.142->llama-index==0.6.0.alpha3) (2.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json->llama-index==0.6.0.alpha3) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install  langchain transformers sentence_transformers \n",
        "\n",
        "!pip install --upgrade llama-index==0.6.0.alpha3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30eKYsljXwbz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSgQvO7deryS"
      },
      "outputs": [],
      "source": [
        "\n",
        "from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTListIndex,GPTVectorStoreIndex, PromptHelper\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import LLMPredictor, ServiceContext\n",
        "import torch\n",
        "from langchain.llms.base import LLM\n",
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTwIZWGkgliA"
      },
      "outputs": [],
      "source": [
        "class customLLM(LLM):\n",
        "    model_name = \"google/flan-t5-large\"\n",
        "    pipeline = pipeline(\"text2text-generation\", model=model_name, device='cpu', model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
        "\n",
        "    def _call(self, prompt, stop=None):\n",
        "        return self.pipeline(prompt, max_length=9999)[0][\"generated_text\"]\n",
        " \n",
        "    def _identifying_params(self):\n",
        "        return {\"name_of_model\": self.model_name}\n",
        "\n",
        "    def _llm_type(self):\n",
        "        return \"custom\"\n",
        "\n",
        "\n",
        "llm_predictor = LLMPredictor(llm=customLLM())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9QtvbyAgllU"
      },
      "outputs": [],
      "source": [
        "hfemb = HuggingFaceEmbeddings()\n",
        "embed_model = LangchainEmbedding(hfemb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWD-ILlp1gb8"
      },
      "outputs": [],
      "source": [
        "text1 = \"\"\"The use of NLP in the realm of financial technology is broad and complex, with applications\n",
        "ranging from sentiment analysis and named entity recognition to question answering. Large\n",
        "Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no\n",
        "LLM specialized for the financial domain has been reported in literature. In this work, we\n",
        "present BloombergGPT, a 50 billion parameter language model that is trained on a wide\n",
        "range of financial data. We construct a 363 billion token dataset based on Bloomberg’s\n",
        "extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345\n",
        "billion tokens from general purpose datasets. We validate BloombergGPT on standard\n",
        "LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most\n",
        "accurately reflect our intended usage. Our mixed dataset training leads to a model that\n",
        "outperforms existing models on financial tasks by significant margins without sacrificing\n",
        "performance on general LLM benchmarks. Additionally, we explain our modeling choices,\n",
        "training process, and evaluation methodology. As a next step, we plan to release training\n",
        "logs (Chronicles) detailing our experience in training BloombergGPT.\n",
        "Snap Inc., the creator of Snapchat, introduced My AI for Snapchat+ this week. The experimental feature is running on ChatGPT API. My AI offers Snapchatters a friendly, customizable chatbot at their fingertips that offers recommendations, and can even write a haiku for friends in seconds. Snapchat, where communication and messaging is a daily behavior, has 750 million monthly Snapchatters.\n",
        "\n",
        "\n",
        "Quizlet Q-Chat, UI screenshot\n",
        "\n",
        "Play video\n",
        "Quizlet Q-Chat\n",
        "\n",
        "Quizlet is a global learning platform with more than 60 million students using it to study, practice and master whatever they’re learning. Quizlet has worked with OpenAI for the last three years, leveraging GPT-3 across multiple use cases, including vocabulary learning and practice tests. With the launch of ChatGPT API, Quizlet is introducing Q-Chat, a fully-adaptive AI tutor that engages students with adaptive questions based on relevant study materials delivered through a fun chat experience.\n",
        "\n",
        "\n",
        "Instacart’s Ask Instacart, UI screenshot\n",
        "Instacart’s Ask Instacart\n",
        "\n",
        "Instacart is augmenting the Instacart app to enable customers to ask about food and get inspirational, shoppable answers. This uses ChatGPT alongside Instacart’s own AI and product data from their 75,000+ retail partner store locations to help customers discover ideas for open-ended shopping goals, such as “How do I make great fish tacos?” or “What’s a healthy lunch for my kids?” Instacart plans to launch “Ask Instacart” later this year.\n",
        "\n",
        "\n",
        "\n",
        "Play video\n",
        "Shopify’s Shop app\n",
        "\n",
        "Shop, Shopify’s consumer app, is used by 100 million shoppers to find and engage with the products and brands they love. ChatGPT API is used to power Shop’s new shopping assistant. When shoppers search for products, the shopping assistant makes personalized recommendations based on their requests. Shop’s new AI-powered shopping assistant will streamline in-app shopping by scanning millions of products to quickly find what buyers are looking for—or help them discover something new.\n",
        "\n",
        "\n",
        "The Speak App, UI screenshot\n",
        "\n",
        "Play video\n",
        "The Speak app\n",
        "\n",
        "Speak is an AI-powered language learning app focused on building the best path to spoken fluency. They’re the fastest-growing English app in South Korea, and are already using the Whisper API to power a new AI speaking companion product, and rapidly bring it to the rest of the globe. Whisper’s human-level accuracy for language learners of every level unlocks true open-ended conversational practice and highly accurate feedback.\n",
        "\n",
        "ChatGPT API\n",
        "Model: The ChatGPT model family we are releasing today, gpt-3.5-turbo, is the same model used in the ChatGPT product. It is priced at \n",
        "OPENAI_API_KEY\"\n",
        "  -H \"Content-Type: application/json\"\n",
        "  -d '{\n",
        "  \"model\": \"gpt-3.5-turbo\",\n",
        "  \"messages\": [{\"role\": \"user\", \"content\": \"What is the OpenAI mission?\"}]\n",
        "}'\n",
        "To learn more about the ChatGPT API, visit our Chat guide.\n",
        "\n",
        "ChatGPT upgrades\n",
        "We are constantly improving our ChatGPT models, and want to make these enhancements available to developers as well. Developers who use the gpt-3.5-turbo model will always get our recommended stable model, while still having the flexibility to opt for a specific model version. For example, today we’re releasing gpt-3.5-turbo-0301, which will be supported through at least June 1st, and we’ll update gpt-3.5-turbo to a new stable release in April. The models page will provide switchover updates.\n",
        "\n",
        "Dedicated instances\n",
        "We are also now offering dedicated instances for users who want deeper control over the specific model version and system performance. By default, requests are run on compute infrastructure shared with other users, who pay per request. Our API runs on Azure, and with dedicated instances, developers will pay by time period for an allocation of compute infrastructure that’s reserved for serving their requests.\n",
        "\n",
        "Developers get full control over the instance’s load (higher load improves throughput but makes each request slower), the option to enable features such as longer context limits, and the ability to pin the model snapshot.\n",
        "\n",
        "Dedicated instances can make economic sense for developers running beyond ~450M tokens per day. Additionally, it enables directly optimizing a developer’s workload against hardware performance, which can dramatically reduce costs relative to shared infrastructure. For dedicated instance inquiries, contact us.\n",
        "\n",
        "Whisper API\n",
        "Whisper, the speech-to-text model we open-sourced in September 2022, has received immense praise from the developer community but can also be hard to run. We’ve now made the large-v2 model available through our API, which gives convenient on-demand access priced at \n",
        "OPENAI_API_KEY\" \\\n",
        "  -H \"Content-Type: multipart/form-data\" \\\n",
        "  -F model=\"whisper-1\" \\\n",
        "  -F file=\"@/path/to/file/openai.mp3\"\n",
        "To learn more about the Whisper API, visit our Speech to Text guide.\n",
        "\n",
        "Developer focus\n",
        "Over the past six months, we’ve been collecting feedback from our API customers to understand how we can better serve them. We’ve made concrete changes, such as:\n",
        "\n",
        "Data submitted through the API is no longer used for service improvements (including model training) unless the organization opts in\n",
        "Implementing a default 30-day data retention policy for API users, with options for stricter retention depending on user needs.\n",
        "Removing our pre-launch review (unlocked by improving our automated monitoring)\n",
        "Improving developer documentation\n",
        "Simplifying our Terms of Service and Usage Policies, including terms around data ownership: users own the input and output of the models.\n",
        "For the past two months our uptime has not met our own expectations nor that of our users. Our engineering team’s top priority is now stability of production use cases—we know that ensuring AI benefits all of humanity requires being a reliable service provider. Please hold us accountable for improved uptime over the upcoming months!\n",
        "\n",
        "We believe that AI can provide incredible opportunities and economic empowerment to everyone, and the best way to achieve that is to allow everyone to build with it. We hope that the changes we announced today will lead to numerous applications that everyone can benefit from. Start building next-generation apps powered by ChatGPT & Whisper.\"\"\"\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8P-hes0A_59",
        "outputId": "280416d9-991e-4a5f-c0b9-99234413ebb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The use of NLP in the realm of financial technology is broad and complex, with applications\n",
            "ranging from sentiment analysis and named entity recognition to question answering. Large\n",
            "Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no\n",
            "LLM specialized for the financial domain has been reported in literature. In this work, we\n",
            "present BloombergGPT, a 50 billion parameter language model that is trained on a wide\n",
            "range of financial data. We construct a 363 billion token dataset based on Bloomberg’s\n",
            "extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345\n",
            "billion tokens from general purpose datasets. We validate BloombergGPT on standard\n",
            "LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most\n",
            "accurately reflect our intended usage. Our mixed dataset training leads to a model that\n",
            "outperforms existing models on financial tasks by significant margins without sacrificing\n",
            "performance on general LLM benchmarks. Additionally, we explain our modeling choices,\n",
            "training process, and evaluation methodology. As a next step, we plan to release training\n",
            "logs (Chronicles) detailing our experience in training BloombergGPT.\n",
            "Snap Inc., the creator of Snapchat, introduced My AI for Snapchat+ this week. The experimental feature is running on ChatGPT API. My AI offers Snapchatters a friendly, customizable chatbot at their fingertips that offers recommendations, and can even write a haiku for friends in seconds. Snapchat, where communication and messaging is a daily behavior, has 750 million monthly Snapchatters.\n",
            "\n",
            "\n",
            "Quizlet Q-Chat, UI screenshot\n",
            "\n",
            "Play video\n",
            "Quizlet Q-Chat\n",
            "\n",
            "Quizlet is a global learning platform with more than 60 million students using it to study, practice and master whatever they’re learning. Quizlet has worked with OpenAI for the last three years, leveraging GPT-3 across multiple use cases, including vocabulary learning and practice tests. With the launch of ChatGPT API, Quizlet is introducing Q-Chat, a fully-adaptive AI tutor that engages students with adaptive questions based on relevant study materials delivered through a fun chat experience.\n",
            "\n",
            "\n",
            "Instacart’s Ask Instacart, UI screenshot\n",
            "Instacart’s Ask Instacart\n",
            "\n",
            "Instacart is augmenting the Instacart app to enable customers to ask about food and get inspirational, shoppable answers. This uses ChatGPT alongside Instacart’s own AI and product data from their 75,000+ retail partner store locations to help customers discover ideas for open-ended shopping goals, such as “How do I make great fish tacos?” or “What’s a healthy lunch for my kids?” Instacart plans to launch “Ask Instacart” later this year.\n",
            "\n",
            "\n",
            "\n",
            "Play video\n",
            "Shopify’s Shop app\n",
            "\n",
            "Shop, Shopify’s consumer app, is used by 100 million shoppers to find and engage with the products and brands they love. ChatGPT API is used to power Shop’s new shopping assistant. When shoppers search for products, the shopping assistant makes personalized recommendations based on their requests. Shop’s new AI-powered shopping assistant will streamline in-app shopping by scanning millions of products to quickly find what buyers are looking for—or help them discover something new.\n",
            "\n",
            "\n",
            "The Speak App, UI screenshot\n",
            "\n",
            "Play video\n",
            "The Speak app\n",
            "\n",
            "Speak is an AI-powered language learning app focused on building the best path to spoken fluency. They’re the fastest-growing English app in South Korea, and are already using the Whisper API to power a new AI speaking companion product, and rapidly bring it to the rest of the globe. Whisper’s human-level accuracy for language learners of every level unlocks true open-ended conversational practice and highly accurate feedback.\n",
            "\n",
            "ChatGPT API\n",
            "Model: The ChatGPT model family we are releasing today, gpt-3.5-turbo, is the same model used in the ChatGPT product. It is priced at \n",
            "OPENAI_API_KEY\"\n",
            "  -H \"Content-Type: application/json\"\n",
            "  -d '{\n",
            "  \"model\": \"gpt-3.5-turbo\",\n",
            "  \"messages\": [{\"role\": \"user\", \"content\": \"What is the OpenAI mission?\"}]\n",
            "}'\n",
            "To learn more about the ChatGPT API, visit our Chat guide.\n",
            "\n",
            "ChatGPT upgrades\n",
            "We are constantly improving our ChatGPT models, and want to make these enhancements available to developers as well. Developers who use the gpt-3.5-turbo model will always get our recommended stable model, while still having the flexibility to opt for a specific model version. For example, today we’re releasing gpt-3.5-turbo-0301, which will be supported through at least June 1st, and we’ll update gpt-3.5-turbo to a new stable release in April. The models page will provide switchover updates.\n",
            "\n",
            "Dedicated instances\n",
            "We are also now offering dedicated instances for users who want deeper control over the specific model version and system performance. By default, requests are run on compute infrastructure shared with other users, who pay per request. Our API runs on Azure, and with dedicated instances, developers will pay by time period for an allocation of compute infrastructure that’s reserved for serving their requests.\n",
            "\n",
            "Developers get full control over the instance’s load (higher load improves throughput but makes each request slower), the option to enable features such as longer context limits, and the ability to pin the model snapshot.\n",
            "\n",
            "Dedicated instances can make economic sense for developers running beyond ~450M tokens per day. Additionally, it enables directly optimizing a developer’s workload against hardware performance, which can dramatically reduce costs relative to shared infrastructure. For dedicated instance inquiries, contact us.\n",
            "\n",
            "Whisper API\n",
            "Whisper, the speech-to-text model we open-sourced in September 2022, has received immense praise from the developer community but can also be hard to run. We’ve now made the large-v2 model available through our API, which gives convenient on-demand access priced at \n",
            "OPENAI_API_KEY\"   -H \"Content-Type: multipart/form-data\"   -F model=\"whisper-1\"   -F file=\"@/path/to/file/openai.mp3\"\n",
            "To learn more about the Whisper API, visit our Speech to Text guide.\n",
            "\n",
            "Developer focus\n",
            "Over the past six months, we’ve been collecting feedback from our API customers to understand how we can better serve them. We’ve made concrete changes, such as:\n",
            "\n",
            "Data submitted through the API is no longer used for service improvements (including model training) unless the organization opts in\n",
            "Implementing a default 30-day data retention policy for API users, with options for stricter retention depending on user needs.\n",
            "Removing our pre-launch review (unlocked by improving our automated monitoring)\n",
            "Improving developer documentation\n",
            "Simplifying our Terms of Service and Usage Policies, including terms around data ownership: users own the input and output of the models.\n",
            "For the past two months our uptime has not met our own expectations nor that of our users. Our engineering team’s top priority is now stability of production use cases—we know that ensuring AI benefits all of humanity requires being a reliable service provider. Please hold us accountable for improved uptime over the upcoming months!\n",
            "\n",
            "We believe that AI can provide incredible opportunities and economic empowerment to everyone, and the best way to achieve that is to allow everyone to build with it. We hope that the changes we announced today will lead to numerous applications that everyone can benefit from. Start building next-generation apps powered by ChatGPT & Whisper.\n"
          ]
        }
      ],
      "source": [
        "print (text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NigC5tM-cj_"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, load_index_from_storage, StorageContext\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb3GLQFjGhmE",
        "outputId": "278e74cc-0811-4309-9864-f87ba395dfdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.9/dist-packages (3.0.1)\n",
            "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhE0GVmt1xSy",
        "outputId": "a0f2eb4d-8855-40df-e9b6-0d439b4bf343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(text='The use of NLP in the realm of financial technology is broad and complex, with applicationsranging from sentiment analysis and named entity recognition to question answering. LargeLanguage Models (LLMs) have been shown to be effective on a variety of tasks; however, noLLM specialized for the financial domain has been reported in literature. In this work, wepresent BloombergGPT, a 50 billion parameter language model that is trained on a widerange of financial data. We construct a 363 billion token dataset based on Bloomberg’sextensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345billion tokens from general purpose datasets. We validate BloombergGPT on standardLLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that mostaccurately reflect our intended usage. Our mixed dataset training leads to a model thatoutperforms existing models on financial tasks by significant margins without sacrificingperformance on general LLM benchmarks. Additionally, we explain our modeling choices,training process, and evaluation methodology. As a next step, we plan to release traininglogs (Chronicles) detailing our experience in training BloombergGPT.Snap Inc., the creator of Snapchat, introduced My AI for Snapchat+ this week. The experimental feature is running on ChatGPT API. My AI offers Snapchatters a friendly, customizable chatbot at their fingertips that offers recommendations, and can even write a haiku for friends in seconds. Snapchat, where communication and messaging is a daily behavior, has 750 million monthly Snapchatters.Quizlet Q-Chat, UI screenshotPlay videoQuizlet Q-ChatQuizlet is a global learning platform with more than 60 million students using it to study, practice and master whatever they’re learning. Quizlet has worked with OpenAI for the last three years, leveraging GPT-3 across multiple use cases, including vocabulary learning and practice tests. With the launch of ChatGPT API, \\nQuizlet is introducing Q-Chat, a fully-adaptive AI tutor that engages students with adaptive questions based on relevant study materials delivered through a fun chat experience.Instacart’s Ask Instacart, UI screenshotInstacart’s Ask InstacartInstacart is augmenting the Instacart app to enable customers to ask about food and get inspirational, shoppable answers. This uses ChatGPT alongside Instacart’s own AI and product data from their 75,000+ retail partner store locations to help customers discover ideas for open-ended shopping goals, such as “How do I make great fish tacos?” or “What’s a healthy lunch for my kids?” Instacart plans to launch “Ask Instacart” later this year.Play videoShopify’s Shop appShop, Shopify’s consumer app, is used by 100 million shoppers to find and engage with the products and brands they love. ChatGPT API is used to power Shop’s new shopping assistant. When shoppers search for products, the shopping assistant makes personalized recommendations based on their requests. Shop’s new AI-powered shopping assistant will streamline in-app shopping by scanning millions of products to quickly find what buyers are looking for—or help them discover something new.The Speak App, UI screenshotPlay videoThe Speak appSpeak is an AI-powered language learning app focused on building the best path to spoken fluency. They’re the fastest-growing English app in South Korea, and are already using the Whisper API to power a new AI speaking companion product, and rapidly bring it to the rest of the globe. Whisper’s human-level accuracy for language learners of every level unlocks true open-ended conversational practice and highly accurate feedback.ChatGPT APIModel: The ChatGPT model family we are releasing today, gpt-3.5-turbo, is the same model used in the ChatGPT product. It is priced at \\nOPENAI_API_KEY\"  -H \"Content-Type: application/json\"  -d \\'{  \"model\": \"gpt-3.5-turbo\",  \"messages\": [{\"role\": \"user\", \"content\": \"What is the OpenAI mission?\"}]}\\'To learn more about the ChatGPT API, visit our Chat guide.ChatGPT upgradesWe are constantly improving our ChatGPT models, and want to make these enhancements available to developers as well. Developers who use the gpt-3.5-turbo model will always get our recommended stable model, while still having the flexibility to opt for a specific model version. For example, today we’re releasing gpt-3.5-turbo-0301, which will be supported through at least June 1st, and we’ll update gpt-3.5-turbo to a new stable release in April. The models page will provide switchover updates.Dedicated instancesWe are also now offering dedicated instances for users who want deeper control over the specific model version and system performance. By default, requests are run on compute infrastructure shared with other users, who pay per request. Our API runs on Azure, and with dedicated instances, developers will pay by time period for an allocation of compute infrastructure that’s reserved for serving their requests.Developers get full control over the instance’s load (higher load improves throughput but makes each request slower), the option to enable features such as longer context limits, and the ability to pin the model snapshot.Dedicated instances can make economic sense for developers running beyond ~450M tokens per day. Additionally, it enables directly optimizing a developer’s workload against hardware performance, which can dramatically reduce costs relative to shared infrastructure. For dedicated instance inquiries, contact us.Whisper APIWhisper, the speech-to-text model we open-sourced in September 2022, has received immense praise from the developer community but can also be hard to run. We’ve now made the large-v2 model available through our API, which gives convenient on-demand access priced at OPENAI_API_KEY\" \\\\  -H \"Content-Type: multipart/form-data\" \\\\  -F model=\"whisper-1\" \\\\  -F file=\"@/path/to/file/openai.mp3\"To learn more about the Whisper API, visit our Speech to Text guide.\\nDeveloper focusOver the past six months, we’ve been collecting feedback from our API customers to understand how we can better serve them. We’ve made concrete changes, such as:Data submitted through the API is no longer used for service improvements (including model training) unless the organization opts inImplementing a default 30-day data retention policy for API users, with options for stricter retention depending on user needs.Removing our pre-launch review (unlocked by improving our automated monitoring)Improving developer documentationSimplifying our Terms of Service and Usage Policies, including terms around data ownership: users own the input and output of the models.For the past two months our uptime has not met our own expectations nor that of our users. Our engineering team’s top priority is now stability of production use cases—we know that ensuring AI benefits all of humanity requires being a reliable service provider. Please hold us accountable for improved uptime over the upcoming months!We believe that AI can provide incredible opportunities and economic empowerment to everyone, and the best way to achieve that is to allow everyone to build with it. We hope that the changes we announced today will lead to numerous applications that everyone can benefit from. Start building next-generation apps powered by ChatGPT & Whisper.', doc_id='6979b21e-8ff2-47cd-8bfb-fd0e5b3c85e6', embedding=None, doc_hash='e9d7855236044b3b8656aee2a95f56516acebb263fd14ebe02e2208fce9fb19f', extra_info=None)]\n"
          ]
        }
      ],
      "source": [
        "from llama_index import Document\n",
        "import PyPDF2\n",
        "documents = SimpleDirectoryReader('data').load_data()\n",
        "\n",
        "from llama_index import Document\n",
        "\n",
        "#text_list = [text1]\n",
        "\n",
        "#documents = [Document(t) for t in text_list]\n",
        "print (documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr0DmLRv2A8C"
      },
      "outputs": [],
      "source": [
        "#index = GPTVectorStoreIndex(documents, embed_model=embed_model, llm_predictor=llm_predictor)\n",
        "\n",
        "#index = GPTListIndex(documents, embed_model=embed_model, llm_predictor=llm_predictor)\n",
        "\n",
        "#index.save_to_disk('index.json')\n",
        "\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)\n",
        "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_L_bf3OB_zO",
        "outputId": "d7d48ae9-4026-4d32-e8ee-57a7c7dce82d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1152 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iii.\n"
          ]
        }
      ],
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What's the cost of Whisper model?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZZPDYlq-DN9"
      },
      "outputs": [],
      "source": [
        "index.set_index_id(\"vector_index\")\n",
        "index.storage_context.persist('storage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f0PZlqTiYv-M",
        "outputId": "872f7e25-8fd3-46ef-dc31-e1dd51ff616e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'iii.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "response.response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXfyrsMu-J4_"
      },
      "outputs": [],
      "source": [
        "# rebuild storage context\n",
        "#storage_context = StorageContext.from_defaults(persist_dir='storage')\n",
        "# load index\n",
        "#index = load_index_from_storage(storage_context, index_id=\"vector_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eNC0vTHj2Stb",
        "outputId": "44eab403-291d-42d1-e736-396d506bec99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting llama-index==0.6.0.alpha3\n",
            "  Downloading llama_index-0.6.0a3.tar.gz (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.0/179.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: dataclasses_json in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (0.5.7)\n",
            "Collecting langchain==0.0.142\n",
            "  Downloading langchain-0.0.142-py3-none-any.whl (548 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.8/548.8 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (1.22.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (8.2.2)\n",
            "Requirement already satisfied: openai>=0.26.4 in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (0.27.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (1.5.3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/dist-packages (from llama-index==0.6.0.alpha3) (0.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (6.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (3.8.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (1.10.7)\n",
            "Collecting SQLAlchemy<2,>=1\n",
            "  Downloading SQLAlchemy-1.4.48-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gptcache>=0.1.7\n",
            "  Downloading gptcache-0.1.23-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.4/82.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (2.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (4.0.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (2.27.1)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.142->llama-index==0.6.0.alpha3) (1.2.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses_json->llama-index==0.6.0.alpha3) (3.19.0)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses_json->llama-index==0.6.0.alpha3) (0.8.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses_json->llama-index==0.6.0.alpha3) (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai>=0.26.4->llama-index==0.6.0.alpha3) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->llama-index==0.6.0.alpha3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->llama-index==0.6.0.alpha3) (2022.7.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken->llama-index==0.6.0.alpha3) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (23.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index==0.6.0.alpha3) (6.0.4)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain==0.0.142->llama-index==0.6.0.alpha3) (5.3.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses_json->llama-index==0.6.0.alpha3) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain==0.0.142->llama-index==0.6.0.alpha3) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index==0.6.0.alpha3) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.142->llama-index==0.6.0.alpha3) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.142->llama-index==0.6.0.alpha3) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.142->llama-index==0.6.0.alpha3) (2022.12.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.142->llama-index==0.6.0.alpha3) (2.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses_json->llama-index==0.6.0.alpha3) (1.0.0)\n",
            "Building wheels for collected packages: llama-index\n",
            "  Building wheel for llama-index (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-index: filename=llama_index-0.6.0a3-py3-none-any.whl size=275777 sha256=c58881e796edbdc98570e9b0944ec70ebc38049507e1d1f64c566b78be9b1c7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/58/5a/9f56672fc327dd065a85f088158aff2bd12ce712599d7c52fb\n",
            "Successfully built llama-index\n",
            "Installing collected packages: SQLAlchemy, gptcache, langchain, llama-index\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.9\n",
            "    Uninstalling SQLAlchemy-2.0.9:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.9\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.168\n",
            "    Uninstalling langchain-0.0.168:\n",
            "      Successfully uninstalled langchain-0.0.168\n",
            "  Attempting uninstall: llama-index\n",
            "    Found existing installation: llama-index 0.6.6\n",
            "    Uninstalling llama-index-0.6.6:\n",
            "      Successfully uninstalled llama-index-0.6.6\n",
            "Successfully installed SQLAlchemy-1.4.48 gptcache-0.1.23 langchain-0.0.142 llama-index-0.6.0a3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "llama_index",
                  "sqlalchemy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import logging\n",
        "\n",
        "logging.getLogger().setLevel(logging.CRITICAL)\n",
        "\n",
        "#!pip install --upgrade llama-index==0.6.0.alpha3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "5lxM_6N82Y1H",
        "outputId": "9e3d96b8-6d8a-4714-f0fc-21cb9bdc80f7"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-c2f573debdbd>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mqueryEngine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_query_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mqueryEngine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Whats the cost of Whisper model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#response=index.query(\"Whats the cost of Whisper model\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/base.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mstr_or_query_bundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     async def aquery(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/llama_index/query_engine/retriever_query_engine.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, query_bundle)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/llama_index/indices/query/response_synthesis.py\u001b[0m in \u001b[0;36msynthesize\u001b[0;34m(self, query_bundle, nodes, additional_source_nodes)\u001b[0m\n\u001b[1;32m    161\u001b[0m             nodes = node_processor.postprocess_nodes(\n\u001b[1;32m    162\u001b[0m                 \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"query_bundle\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             )\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mtext_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\u001b[0m in \u001b[0;36mget_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[1;32m    294\u001b[0m             response = super().get_response(\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mquery_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             )\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/llama_index/token_counter/token_counter.py\u001b[0m in \u001b[0;36mwrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_llm_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwrapper_logic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mf_return_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf_return_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\u001b[0m in \u001b[0;36mget_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# if this is the first chunk, and text chunk already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# is an answer, then return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 response = self._give_response_single(\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0mquery_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mtext_chunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/llama_index/indices/response/response_builder.py\u001b[0m in \u001b[0;36m_give_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mformatted_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_service_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtext_qa_template\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                     \u001b[0mcontext_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_text_chunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mprompt\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPrompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPrompt\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mllm_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfull_prompt_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mllm_prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPrompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprompt_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/llama_index/utils.py\u001b[0m in \u001b[0;36mretry_on_exceptions_with_backoff\u001b[0;34m(lambda_fn, errors_to_retry, max_tries, min_backoff_secs, max_backoff_secs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlambda_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexception_class_tuples\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/llama_index/llm_predictor/base.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mllm_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfull_prompt_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mllm_prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPrompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprompt_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \"\"\"Predict the answer to a query.\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_chain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"llm_chain\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             )\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             self.callback_manager.on_chain_start(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_async\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             await self.callback_manager.on_chain_start(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magenerate_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     def prep_prompts(\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     ) -> Tuple[List[PromptValue], Optional[List[str]]]:\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mselected_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mselected_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0m_colored_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_colored_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"green\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0m_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Prompt after formatting:\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_colored_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 )\n\u001b[1;32m    133\u001b[0m             self.callback_manager.on_llm_start(\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             )\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             llm_output = update_cache(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mdict\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     ) -> LLMResult:\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0;34m\"\"\"Run the LLM on the given prompt and input.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mgenerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
          ]
        }
      ],
      "source": [
        "queryEngine = index.as_query_engine()\n",
        "\n",
        "response=queryEngine.query(\"Whats the cost of Whisper model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kztxK94B2fyo"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YIv6Gzxs2l8V"
      },
      "outputs": [],
      "source": [
        "# temp fix for running shell commands on Google Colab\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPeAS4rv2p1j",
        "outputId": "c5944c56-b59f-4f62-ffa0-9c2cf921408e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.9/287.9 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BC3NR5kM2uBg"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "index = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gw_x4GPy25VO"
      },
      "outputs": [],
      "source": [
        "def build_the_bot(input_text):\n",
        "  text_list = [input_text]\n",
        "  documents = [Document(t) for t in text_list]\n",
        "  global index\n",
        "  service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)\n",
        "  index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "  return('Index saved successfull!!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eQbw4Xn7286m"
      },
      "outputs": [],
      "source": [
        "def chat(chat_history, user_input):\n",
        "  \n",
        "  #bot_response = index.query(user_input)\n",
        "  queryEngine = index.as_query_engine()\n",
        "\n",
        "  bot_response=queryEngine.query(user_input)\n",
        "\n",
        "\n",
        "  print(bot_response)\n",
        "  response = \"\"\n",
        "  for letter in ''.join(bot_response.response): #[bot_response[i:i+1] for i in range(0, len(bot_response), 1)]:\n",
        "      response += letter + \"\"\n",
        "      yield chat_history + [(user_input, response)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vX6Oq9hX3AiF",
        "outputId": "1efdb3ce-d753-4f68-841b-c5b5060fdc54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://6ee86aca7bb2936904.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6ee86aca7bb2936904.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gradio/routes.py\", line 414, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gradio/blocks.py\", line 1320, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gradio/blocks.py\", line 1064, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gradio/utils.py\", line 514, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gradio/utils.py\", line 507, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/gradio/utils.py\", line 490, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"<ipython-input-20-9cb26b4dfe86>\", line 4, in chat\n",
            "    queryEngine = index.as_query_engine()\n",
            "AttributeError: 'NoneType' object has no attribute 'as_query_engine'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Business\n",
            "Our commitment to corporate clients spans our 150 year history, and creates powerful momentum for the future\n",
            "Our commitment to corporate clients spans our 150 year history, and creates powerful momentum for the future\n",
            "DDA ,CDA and REA\n",
            "Wells Fargo Corporate & Investment Banking supports clients through dedicated industry and specialty coverage teams with the knowledge and services you need today—and the vision and strategic capital that tomorrow’s goals demand. Our experienced bankers understand your market, advise with candor, and take pride in lasting, collaborative relationships. Our commitment to corporate clients spans our 150 year history, and creates powerful momentum for the future. We align our industry focused bankers with top companies in these sectors: consumer and retail; energy and power; financial institutions; healthcare; industrials; real estate; and technology, media, and telecom. Dedicated to your success through deep industry knowledge, capital, and understanding where you need to go.\n",
            "Dedicated to your success through deep industry knowledge and capital\n",
            "Our experienced bankers understand your market, advise with candor, and take pride in lasting, collaborative relationships. Our commitment to corporate clients spans our 150 year history, and creates powerful momentum for the future. We align our industry focused bankers with top companies in these sectors: consumer and retail; energy and power; financial institutions; healthcare; industrials; real estate; and technology, media, and telecom. Dedicated to your success through deep industry knowledge, capital, and understanding of where you need to go.\n"
          ]
        }
      ],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown('# Q&A Bot with Hugging Face Models')\n",
        "    with gr.Tab(\"Input Text Document\"):\n",
        "        text_input = gr.Textbox()\n",
        "        text_output = gr.Textbox()\n",
        "        text_button = gr.Button(\"Build the Bot!!!\")\n",
        "        text_button.click(build_the_bot, text_input, text_output)\n",
        "    with gr.Tab(\"Knowledge Bot\"):\n",
        "#          inputbox = gr.Textbox(\"Input your text to build a Q&A Bot here.....\")\n",
        "          chatbot = gr.Chatbot()\n",
        "          message = gr.Textbox (\"What is this document about?\")\n",
        "          message.submit(chat, [chatbot, message], chatbot)\n",
        "\n",
        "demo.queue().launch(debug = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abCrZHEo3bmi"
      },
      "source": [
        "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
        "\n",
        "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
        "Running on public URL: https://7e34d8e6b2adf10738.gradio.live\n",
        "\n",
        "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n",
        "Keyboard interruption in main thread... closing server.\n",
        "Killing tunnel 127.0.0.1:7860 <> https://7e34d8e6b2adf10738.gradio.live"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKCp24jBNbIO7u7sTUluis",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}