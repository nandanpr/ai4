{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is: wavesproject\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"${PROJECT}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open '/tmp/waves_data/waves-validation-AI.csv' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT = \"wavesproject\"  # Replace with your project name\n",
    "BUCKET_NAME=\"wavesbucket-2\"\n",
    "\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "'''\n",
    "gs://wavesbucket-2\n",
    "https://console.cloud.google.com/storage/browser/wavesbucket-2\n",
    "\n",
    "https://storage.googleapis.com/wavesbucket-2/wavesAI.csv\n",
    "\n",
    "https://storage.googleapis.com/wavesbucket-2/waves-validation-AI.csv\n",
    "'''\n",
    "\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET_NAME\"] = BUCKET_NAME\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.3\"\n",
    "os.environ[\"PYTHONVERSION\"] = \"3.7\"\n",
    "#Ensure to remove temporary files\n",
    "!rm -f /tmp/waves_data/waves-validation-AI.csv\n",
    "!rm -f /tmp/waves_data/wavesAI.csv\n",
    "!head -5 /tmp/waves_data/waves-validation-AI.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if ! gsutil ls | grep -q gs://${BUCKET_NAME}; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET_NAME}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://wavesbucket-2/wavesAI.csv...\n",
      "/ [1 files][  3.0 KiB/  3.0 KiB]                                                \n",
      "Operation completed over 1 objects/3.0 KiB.                                      \n",
      "Copying gs://wavesbucket-2/waves-validation-AI.csv...\n",
      "/ [1 files][  2.0 KiB/  2.0 KiB]                                                \n",
      "Operation completed over 1 objects/2.0 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil cp  gs://$BUCKET_NAME/wavesAI.csv .\n",
    "gsutil cp  gs://$BUCKET_NAME/waves-validation-AI.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p trainer\n",
    "touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/util.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Storage directory\n",
    "DATA_DIR = os.path.join(tempfile.gettempdir(), 'waves_data')\n",
    "\n",
    "# Download options.\n",
    "DATA_URL = (\n",
    "    'https://storage.googleapis.com/wavesbucket-2'\n",
    "    )\n",
    "TRAINING_FILE = 'wavesAI.csv'\n",
    "EVAL_FILE = 'waves-validation-AI.csv'\n",
    "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
    "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)\n",
    "\n",
    "# These are the features in the dataset.\n",
    "\n",
    "_CSV_COLUMNS = [\n",
    "    'ECN', 'ATT1', 'ATT2', 'ATT3', 'ATT4','ATT5',\n",
    "    'ATT6', 'ATT7', 'ATT8', 'ATT9', 'ATT10','ATT11',\n",
    "    'ATT12', 'ATT13', 'ATT14', 'ATT15','ATT16','ATT17',\n",
    "    'ATT18','ATT19','ATT20','gender','workclass','marital_status',\n",
    "    'occupation','relationship','race','native_country','income_bracket',\n",
    "    'ATT_CODE'\n",
    "]\n",
    "\n",
    "# This is the label (target) we want to predict.\n",
    "_LABEL_COLUMN = 'ATT_CODE'\n",
    "\n",
    "# These are columns we will not use as features for training. There are many\n",
    "# reasons not to use certain attributes of data for training. Perhaps their\n",
    "# values are noisy or inconsistent, or perhaps they encode bias that we do not\n",
    "# want our model to learn. For a deep dive into the features of this Census\n",
    "# dataset and the challenges they pose, see the Introduction to ML Fairness\n",
    "# Notebook: https://colab.research.google.com/github/google/eng-edu/blob\n",
    "# /master/ml/cc/exercises/intro_to_fairness.ipynb\n",
    "\n",
    "UNUSED_COLUMNS = ['gender']\n",
    "\n",
    "_CATEGORICAL_TYPES = {\n",
    "    'workclass': pd.api.types.CategoricalDtype(categories=[\n",
    "        'Federal-gov', 'Local-gov', 'Never-worked', 'Private', 'Self-emp-inc',\n",
    "        'Self-emp-not-inc', 'State-gov', 'Without-pay'\n",
    "    ]),\n",
    "    'marital_status': pd.api.types.CategoricalDtype(categories=[\n",
    "        'Divorced', 'Married-AF-spouse', 'Married-CIV-spouse',\n",
    "        'Married-spouse-absent', 'Never-married','Married', 'UnMarried,','Separated', 'Widowed'\n",
    "    ]),\n",
    "    'occupation': pd.api.types.CategoricalDtype([\n",
    "        'Adm-clerical', 'Armed-Forces', 'Craft-repair', 'Exec-managerial',\n",
    "        'Farming-fishing', 'Handlers-cleaners', 'Machine-op-inspct',\n",
    "        'Other-service', 'Priv-house-serv', 'Prof-specialty', 'Protective-serv',\n",
    "        'Sales', 'Prof,','Tech-support', 'Transport-moving'\n",
    "    ]),\n",
    "    'relationship': pd.api.types.CategoricalDtype(categories=[\n",
    "        'Husband', 'Not-in-family', 'Other-relative', 'Own-child', 'Unmarried',\n",
    "        'Wife'\n",
    "    ]),\n",
    "    'race': pd.api.types.CategoricalDtype(categories=[\n",
    "        'Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Asian','Black', 'Other', 'White'\n",
    "    ]),\n",
    "    'native_country': pd.api.types.CategoricalDtype(categories=[\n",
    "        'Cambodia', 'Canada', 'China', 'Columbia', 'Cuba', 'Dominican-Republic',\n",
    "        'Ecuador', 'El-Salvador', 'England', 'France', 'Germany', 'Greece',\n",
    "        'Guatemala', 'Haiti', 'Holand-Netherlands', 'Honduras', 'Hong',\n",
    "        'Hungary',\n",
    "        'India', 'Iran', 'Ireland', 'Italy', 'Jamaica', 'Japan', 'Laos',\n",
    "        'Mexico',\n",
    "        'Nicaragua', 'Outlying-US(Guam-USVI-etc)', 'Peru', 'Philippines',\n",
    "        'Poland',\n",
    "        'Portugal', 'Puerto-Rico', 'Scotland', 'South', 'Taiwan', 'Thailand',\n",
    "        'Trinadad&Tobago', 'United-States', 'Vietnam', 'Yugoslavia'\n",
    "    ]),\n",
    "    'income_bracket': pd.api.types.CategoricalDtype(categories=[\n",
    "        '<=50K', '>50K'\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "def _download_and_clean_file(filename, url):\n",
    "    \"\"\"Downloads data from url, and makes changes to match the CSV format.\n",
    "\n",
    "    The CSVs may use spaces after the comma delimters (non-standard) or include\n",
    "    rows which do not represent well-formed examples. This fUSEDction strips out\n",
    "    some of these problems.\n",
    "\n",
    "    Args:\n",
    "      filename: filename to save url to\n",
    "      url: URL of resource to download\n",
    "    \"\"\"\n",
    "    temp_file, _ = urllib.request.urlretrieve(url)\n",
    "    with tf.io.gfile.GFile(temp_file, 'r') as temp_file_object:\n",
    "        with tf.io.gfile.GFile(filename, 'w') as file_object:\n",
    "            for line in temp_file_object:\n",
    "                line = line.strip()\n",
    "                line = line.replace(', ', ',')\n",
    "                if not line or ',' not in line:\n",
    "                    continue\n",
    "                if line[-1] == '.':\n",
    "                    line = line[:-1]\n",
    "                line += '\\n'\n",
    "                file_object.write(line)\n",
    "    tf.io.gfile.remove(temp_file)\n",
    "\n",
    "\n",
    "def download(data_dir):\n",
    "    \"\"\"Downloads census data if it is not already present.\n",
    "\n",
    "    Args:\n",
    "      data_dir: directory where we will access/save the census data\n",
    "    \"\"\"\n",
    "    print ('In download data_dir is ',data_dir)\n",
    "    \n",
    "    tf.io.gfile.makedirs(data_dir)\n",
    "\n",
    "    training_file_path = os.path.join(data_dir,TRAINING_FILE )\n",
    "    \n",
    "    print (\"Returning training file path in download(datadir) method  \",training_file_path)   \n",
    "    \n",
    "    \n",
    "    if not tf.io.gfile.exists(training_file_path):\n",
    "        _download_and_clean_file(training_file_path, TRAINING_URL)\n",
    "\n",
    "    eval_file_path = os.path.join(data_dir, EVAL_FILE)\n",
    "    if not tf.io.gfile.exists(eval_file_path):\n",
    "        _download_and_clean_file(eval_file_path, EVAL_URL)\n",
    "\n",
    "    print (\"Returning training file path in download(datadir) method  \",training_file_path)     \n",
    "    return training_file_path, eval_file_path\n",
    "\n",
    "\n",
    "def preprocess(dataframe):\n",
    "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "    Args:\n",
    "      dataframe: Pandas dataframe with raw data\n",
    "\n",
    "    Returns:\n",
    "      Dataframe with preprocessed data\n",
    "    \"\"\"\n",
    "    \n",
    "    #Remove gender to take care of bias\n",
    "    dataframe = dataframe.drop(columns=UNUSED_COLUMNS)\n",
    "    \n",
    "    '''\n",
    "    [\n",
    "    'ECN', 'ATT1', 'ATT2', 'ATT3', 'ATT4','ATT5',\n",
    "    'ATT6', 'ATT7', 'ATT8', 'ATT9', 'ATT10','ATT11',\n",
    "    'ATT12', 'ATT13', 'ATT14', 'ATT15','ATT16','ATT17',\n",
    "    'ATT18','ATT19','ATT20','workclass','marital_status',\n",
    "    'occupation','relationship','race','native_country','income_bracket',\n",
    "    'ATT_CODE'\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = dataframe.select_dtypes(['int64']).columns\n",
    "    \n",
    "    numeric_columns=['ECN', 'ATT1', 'ATT2', 'ATT3', 'ATT4','ATT5',\n",
    "    'ATT6', 'ATT7', 'ATT8', 'ATT9', 'ATT10','ATT11',\n",
    "    'ATT12', 'ATT13', 'ATT14', 'ATT15','ATT16','ATT17',\n",
    "    'ATT18','ATT19','ATT20','ATT_CODE']\n",
    "    \n",
    "    print(numeric_columns)\n",
    "    dataframe[numeric_columns] = dataframe[numeric_columns].astype('float32')\n",
    "\n",
    "    # Convert categorical columns to numeric\n",
    "    cat_columns = dataframe.select_dtypes(['object']).columns\n",
    "    \n",
    "    cat_columns=['workclass','marital_status',\n",
    "    'occupation','relationship','race','native_country','income_bracket']\n",
    "    \n",
    "    \n",
    "    dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.astype(\n",
    "        _CATEGORICAL_TYPES[x.name]))\n",
    "    dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def standardize(dataframe):\n",
    "    \"\"\"Scales numerical columns using their means and standard deviation to get\n",
    "    z-scores: the mean of each numerical column becomes 0, and the standard\n",
    "    deviation becomes 1. This can help the model converge during training.\n",
    "\n",
    "    Args:\n",
    "      dataframe: Pandas dataframe\n",
    "\n",
    "    Returns:\n",
    "      Input dataframe with the numerical columns scaled to z-scores\n",
    "    \"\"\"\n",
    "   \n",
    "    dtypes = list(zip(dataframe.dtypes.index, map(str, dataframe.dtypes)))\n",
    "   \n",
    "    # Normalize numeric columns.\n",
    "    for column, dtype in dtypes:\n",
    "        if dtype == 'float32':\n",
    "            dataframe[column] -= dataframe[column].mean()\n",
    "            dataframe[column] /= dataframe[column].std()\n",
    "            \n",
    "                    \n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    for column, dtype in dtypes:\n",
    "        if dtype == 'float32':\n",
    "              dataframe[column] = np.asarray(dataframe[column]).astype('float32').reshape((-1, 1))\n",
    "   \n",
    "              dataframe[column]=scaler.fit_transform(dataframe[column].values)\n",
    "           \n",
    "    \"\"\"\n",
    "    dataframe=dataframe.replace(np.nan, 0)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Loads data into preprocessed (train_x, train_y, eval_y, eval_y)\n",
    "    dataframes.\n",
    "\n",
    "    Returns:\n",
    "      A tuple (train_x, train_y, eval_x, eval_y), where train_x and eval_x are\n",
    "      Pandas dataframes with features for training and train_y and eval_y are\n",
    "      numpy arrays with the corresponding labels.\n",
    "    \"\"\"\n",
    "    # Download Census dataset: Training and eval csv files.\n",
    "    training_file_path, eval_file_path = download(DATA_DIR)\n",
    "    \n",
    "    #training_file_path='waves.csv'\n",
    "    \n",
    "    #eval_file_path='waves-validation.csv'\n",
    "\n",
    "    # This census data uses the value '?' for missing entries. We use\n",
    "    # na_values to\n",
    "    # find ? and set it to NaN.\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv\n",
    "    # .html\n",
    "    \n",
    "    print(training_file_path)\n",
    "    print(eval_file_path)\n",
    "    train_df = pd.read_csv(training_file_path, names=_CSV_COLUMNS,header=0,\n",
    "                           na_values='?')\n",
    "    eval_df = pd.read_csv(eval_file_path, names=_CSV_COLUMNS, header=0, na_values='?')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print (train_df.iloc[0:1,:])\n",
    "    \n",
    "    train_df = preprocess(train_df)\n",
    "    eval_df = preprocess(eval_df)\n",
    "\n",
    "    # Split train and eval data with labels. The pop method copies and removes\n",
    "    # the label column from the dataframe.\n",
    "    train_x, train_y = train_df, train_df.pop(_LABEL_COLUMN)\n",
    "    eval_x, eval_y = eval_df, eval_df.pop(_LABEL_COLUMN)\n",
    "\n",
    "    # Join train_x and eval_x to normalize on overall means and standard\n",
    "    # deviations. Then separate them again.\n",
    "    all_x = pd.concat([train_x, eval_x], keys=['train', 'eval'])\n",
    "    all_x = standardize(all_x)\n",
    "    train_x, eval_x = all_x.xs('train'), all_x.xs('eval')\n",
    "\n",
    "    # Reshape label columns for use with tf.data.Dataset\n",
    "   \n",
    "    #train_y=standardize(train_y)\n",
    "    #eval_y=standardize(eval_y)\n",
    "    '''\n",
    "    all_y = pd.concat([train_y, eval_y], keys=['trainy', 'evaly'])\n",
    "    print(all_y)\n",
    "    print(eval_x)\n",
    "    all_y = standardize(all_y)\n",
    "    train_y, eval_y = all_y.xs('trainy'), all_x.xs('evaly')\n",
    "    '''\n",
    "    \n",
    "    train_y = np.asarray(train_y).astype('float32').reshape((-1, 1))\n",
    "    eval_y = np.asarray(eval_y).astype('float32').reshape((-1, 1))\n",
    "    #Need to standardise this as well  \n",
    "    scaler = MinMaxScaler()\n",
    "    train_y=scaler.fit_transform(train_y)\n",
    "    eval_y=scaler.fit_transform(eval_y)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return train_x, train_y, eval_x, eval_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /tmp/waves_data/wavesAI.csv: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! cat /tmp/waves_data/wavesAI.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/model.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def input_fn(features, labels, shuffle, num_epochs, batch_size):\n",
    "    \"\"\"Generates an input function to be used for model training.\n",
    "\n",
    "    Args:\n",
    "      features: numpy array of features used for training or inference\n",
    "      labels: numpy array of labels for each example\n",
    "      shuffle: boolean for whether to shuffle the data or not (set True for\n",
    "        training, False for evaluation)\n",
    "      num_epochs: number of epochs to provide the data for\n",
    "      batch_size: batch size for training\n",
    "\n",
    "    Returns:\n",
    "      A tf.data.Dataset that can provide data to the Keras model for training or\n",
    "        evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    #Reshape for LSTM 3D tensor\n",
    "    features = features.reshape(features.shape[0], features.shape[1], 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if labels is None:\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "        \n",
    "        \n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(features))\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_keras_model(input_dim, learning_rate):\n",
    "    \"\"\"Creates Keras Model for Binary Classification.\n",
    "\n",
    "    The single output node + Sigmoid activation makes this a Logistic\n",
    "    Regression.\n",
    "\n",
    "    Args:\n",
    "      input_dim: How many features the input has\n",
    "      learning_rate: Learning rate for training\n",
    "\n",
    "    Returns:\n",
    "      The compiled Keras model (still needs to be trained)\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    session = tf.keras.backend.get_session()\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    \"\"\"\n",
    "    tf.config.experimental_run_functions_eagerly(True)\n",
    "    \n",
    "    \n",
    "    Dense = tf.keras.layers.Dense\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "             tf.keras.layers.LSTM(units=30, input_shape=(input_dim,1)),\n",
    "             Dense(1, activation=tf.nn.sigmoid)\n",
    "        ])\n",
    "\n",
    "    # Custom Optimizer:\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate)\n",
    "\n",
    "    # Compile Keras model\n",
    "    model.compile(\n",
    "        loss='mae', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "from . import util\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='local or GCS location for writing checkpoints and exporting '\n",
    "             'models')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        type=int,\n",
    "        default=30,\n",
    "        help='number of times to go through the data, default=20')\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        default=4,\n",
    "        type=int,\n",
    "        help='number of records to read during each training step, default=128')\n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        default=.01,\n",
    "        type=float,\n",
    "        help='learning rate for gradient descent, default=.01')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='INFO')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "\n",
    "    Args:\n",
    "      args: dictionary of arguments - see get_args() for details\n",
    "    \"\"\"\n",
    "\n",
    "    train_x, train_y, eval_x, eval_y = util.load_data()\n",
    "    \n",
    "    # dimensions\n",
    "    num_train_examples, input_dim = train_x.shape\n",
    "    num_eval_examples = eval_x.shape[0]\n",
    "\n",
    "    # Create the Keras Model\n",
    "    keras_model = model.create_keras_model(\n",
    "        input_dim=input_dim, learning_rate=args.learning_rate)\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "    \n",
    "    # Pass a numpy array by passing DataFrame.values\n",
    "    training_dataset = model.input_fn(\n",
    "        features=train_x.values,\n",
    "        labels=train_y,\n",
    "        shuffle=True,\n",
    "        num_epochs=args.num_epochs,\n",
    "        batch_size=args.batch_size)\n",
    "\n",
    "    # Pass a numpy array by passing DataFrame.values\n",
    "    validation_dataset = model.input_fn(\n",
    "        features=eval_x.values,\n",
    "        labels=eval_y,\n",
    "        shuffle=False,\n",
    "        num_epochs=args.num_epochs,\n",
    "        batch_size=num_eval_examples)\n",
    "\n",
    "    # Setup Learning Rate decay.\n",
    "    lr_decay_cb = tf.keras.callbacks.LearningRateScheduler(\n",
    "        lambda epoch: args.learning_rate + 0.02 * (0.5 ** (1 + epoch)),\n",
    "        verbose=True)\n",
    "\n",
    "    # Setup TensorBoard callback.\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "        os.path.join(args.job_dir, 'keras_tensorboard'),\n",
    "        histogram_freq=0)\n",
    "    \n",
    "    \n",
    "    #init = tf.global_variables_initializer()\n",
    "    #tf.config.experimental_run_functions_eagerly(True)\n",
    "    \n",
    "    \n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    first_graph = tf.Graph()\n",
    "    first_session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "    with first_session.as_default():\n",
    "      # Train model\n",
    "        keras_model.fit(\n",
    "            training_dataset,\n",
    "            steps_per_epoch=int(num_train_examples / args.batch_size),\n",
    "            epochs=args.num_epochs,\n",
    "            validation_data=validation_dataset,\n",
    "            validation_steps=1,\n",
    "            verbose=1,\n",
    "            callbacks=[lr_decay_cb, tensorboard_cb])\n",
    "\n",
    "    keras_model.run_eagerly =True\n",
    "    \n",
    "    export_path = os.path.join(args.job_dir, 'keras_export')\n",
    "    tf.keras.models.save_model(keras_model, export_path)\n",
    "    print('Model exported to: {}'.format(export_path))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        first_graph = tf.Graph()\n",
    "        first_session = tf.compat.v1.Session(config=config)\n",
    "        with first_session.as_default(): \n",
    "            args = get_args()\n",
    "            tf.compat.v1.logging.set_verbosity(args.verbosity)\n",
    "            train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open '/tmp/waves_data/waves-validation-AI.csv' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#Ensure to remove temporary files\n",
    "!rm -f /tmp/waves_data/waves-validation-AI.csv\n",
    "!rm -f /tmp/waves_data/wavesAI.csv\n",
    "!head -5 /tmp/waves_data/waves-validation-AI.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ECN  ATT1  ATT2  ATT3  ATT4  ATT5  ATT6  ATT7  ATT8  ATT9  ...  ATT20  \\\n",
      "0    1    90    40    20    67    43    56    78    27    78  ...     55   \n",
      "\n",
      "   gender    workclass        marital_status         occupation  relationship  \\\n",
      "0  'Male'  'State-gov'  'Married-civ-spouse'  'Priv-house-serv'     'Husband'   \n",
      "\n",
      "      race  native_country  income_bracket  ATT_CODE  \n",
      "0  'Black'            'US'          '>50K'     11035  \n",
      "\n",
      "[1 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "_CSV_COLUMNS = [\n",
    "        'ECN', 'ATT1', 'ATT2', 'ATT3', 'ATT4', 'ATT5',\n",
    "        'ATT6', 'ATT7', 'ATT8', 'ATT9', 'ATT10', 'ATT11',\n",
    "        'ATT12', 'ATT13', 'ATT14', 'ATT15', 'ATT16', 'ATT17', \n",
    "         'ATT18', 'ATT19', 'ATT20','gender','workclass','marital_status',\n",
    "         'occupation','relationship','race','native_country','income_bracket',\n",
    "         'ATT_CODE'\n",
    "    ]\n",
    "train_df = pd.read_csv(\"wavesAI.csv\", names=_CSV_COLUMNS,header=0,skiprows=1, na_values='?')\n",
    "    \n",
    "\n",
    "print (train_df.iloc[0:1,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In download data_dir is  /tmp/waves_data\n",
      "Returning training file path in download(datadir) method   /tmp/waves_data/wavesAI.csv\n",
      "Returning training file path in download(datadir) method   /tmp/waves_data/wavesAI.csv\n",
      "/tmp/waves_data/wavesAI.csv\n",
      "/tmp/waves_data/waves-validation-AI.csv\n",
      "   ECN  ATT1  ATT2  ...  native_country  income_bracket  ATT_CODE\n",
      "0    0    70    20  ...        'Canada'         '<=50K'     10005\n",
      "\n",
      "[1 rows x 30 columns]\n",
      "['ECN', 'ATT1', 'ATT2', 'ATT3', 'ATT4', 'ATT5', 'ATT6', 'ATT7', 'ATT8', 'ATT9', 'ATT10', 'ATT11', 'ATT12', 'ATT13', 'ATT14', 'ATT15', 'ATT16', 'ATT17', 'ATT18', 'ATT19', 'ATT20', 'ATT_CODE']\n",
      "['ECN', 'ATT1', 'ATT2', 'ATT3', 'ATT4', 'ATT5', 'ATT6', 'ATT7', 'ATT8', 'ATT9', 'ATT10', 'ATT11', 'ATT12', 'ATT13', 'ATT14', 'ATT15', 'ATT16', 'ATT17', 'ATT18', 'ATT19', 'ATT20', 'ATT_CODE']\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.02.\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.2966 - accuracy: 0.0000e+00 - val_loss: 0.2880 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.015.\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.2794 - accuracy: 0.0500 - val_loss: 0.2773 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.2751 - accuracy: 0.0500 - val_loss: 0.2734 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01125.\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 0.2755 - accuracy: 0.0500 - val_loss: 0.2730 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.010625.\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.2648 - accuracy: 0.0500 - val_loss: 0.2699 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0103125.\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.2648 - accuracy: 0.0500 - val_loss: 0.2673 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.01015625.\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2562 - accuracy: 0.0500 - val_loss: 0.2644 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.010078125.\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2518 - accuracy: 0.1000 - val_loss: 0.2784 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0100390625.\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2876 - accuracy: 0.0500 - val_loss: 0.2727 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.01001953125.\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.2899 - accuracy: 0.0000e+00 - val_loss: 0.2543 - val_accuracy: 0.1667\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.010009765625.\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.2309 - accuracy: 0.1000 - val_loss: 0.2646 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.010004882812500001.\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2457 - accuracy: 0.1000 - val_loss: 0.2323 - val_accuracy: 0.1667\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.01000244140625.\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.2127 - accuracy: 0.1000 - val_loss: 0.2696 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.010001220703125.\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2100 - accuracy: 0.1000 - val_loss: 0.2620 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0100006103515625.\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.2125 - accuracy: 0.1000 - val_loss: 0.3169 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.01000030517578125.\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1832 - accuracy: 0.1000 - val_loss: 0.3334 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.010000152587890625.\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2072 - accuracy: 0.1000 - val_loss: 0.3116 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.010000076293945313.\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1708 - accuracy: 0.1000 - val_loss: 0.2710 - val_accuracy: 0.1667\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.010000038146972657.\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.1682 - accuracy: 0.1000 - val_loss: 0.3668 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.010000019073486329.\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1854 - accuracy: 0.1000 - val_loss: 0.2595 - val_accuracy: 0.1667\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.010000009536743164.\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1650 - accuracy: 0.1000 - val_loss: 0.2821 - val_accuracy: 0.1667\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.010000004768371581.\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1451 - accuracy: 0.1000 - val_loss: 0.3483 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.010000002384185792.\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1433 - accuracy: 0.1000 - val_loss: 0.3117 - val_accuracy: 0.1667\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.010000001192092895.\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1515 - accuracy: 0.1000 - val_loss: 0.3131 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.010000000596046449.\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.1341 - accuracy: 0.1000 - val_loss: 0.2866 - val_accuracy: 0.1667\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.010000000298023224.\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1718 - accuracy: 0.1000 - val_loss: 0.3008 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.010000000149011612.\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.1526 - accuracy: 0.1000 - val_loss: 0.2603 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.010000000074505806.\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1770 - accuracy: 0.1000 - val_loss: 0.3375 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.010000000037252902.\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.1416 - accuracy: 0.1000 - val_loss: 0.3493 - val_accuracy: 0.0833\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.010000000018626451.\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.1283 - accuracy: 0.1000 - val_loss: 0.3435 - val_accuracy: 0.0833\n",
      "Model exported to: output/keras_export\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-04 02:45:48.612959: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200140000 Hz\n",
      "2021-01-04 02:45:48.613621: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ac5956a140 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-01-04 02:45:48.613660: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-01-04 02:45:48.613939: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "2021-01-04 02:45:48.616440: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:From /home/jupyter/trainer/model.py:68: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n",
      "2021-01-04 02:45:49.133075: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "2021-01-04 02:45:49.353055: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2021-01-04 02:45:49.483951: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: output/keras_tensorboard/train/plugins/profile/2021_01_04_02_45_49\n",
      "2021-01-04 02:45:49.512130: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to output/keras_tensorboard/train/plugins/profile/2021_01_04_02_45_49/tensorflow-2-3-20201231-165221.trace.json.gz\n",
      "2021-01-04 02:45:49.523145: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: output/keras_tensorboard/train/plugins/profile/2021_01_04_02_45_49\n",
      "2021-01-04 02:45:49.523818: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to output/keras_tensorboard/train/plugins/profile/2021_01_04_02_45_49/tensorflow-2-3-20201231-165221.memory_profile.json.gz\n",
      "2021-01-04 02:45:49.524404: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: output/keras_tensorboard/train/plugins/profile/2021_01_04_02_45_49Dumped tool data for xplane.pb to output/keras_tensorboard/train/plugins/profile/2021_01_04_02_45_49/tensorflow-2-3-20201231-165221.xplane.pb\n",
      "Dumped tool data for overview_page.pb to output/keras_tensorboard/train/plugins/profile/2021_01_04_02_45_49/tensorflow-2-3-20201231-165221.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to output/keras_tensorboard/train/plugins/profile/2021_01_04_02_45_49/tensorflow-2-3-20201231-165221.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to output/keras_tensorboard/train/plugins/profile/2021_01_04_02_45_49/tensorflow-2-3-20201231-165221.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to output/keras_tensorboard/train/plugins/profile/2021_01_04_02_45_49/tensorflow-2-3-20201231-165221.kernel_stats.pb\n",
      "\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2021-01-04 02:46:08.408327: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: output/keras_export/assets\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_DIR=output\n",
    "gcloud ai-platform local train \\\n",
    "    --module-name trainer.task \\\n",
    "    --package-path trainer/ \\\n",
    "    --job-dir $MODEL_DIR \\\n",
    "    -- \\\n",
    "    --train-files $TRAIN_DATA \\\n",
    "    --eval-files $EVAL_DATA \\\n",
    "    --train-steps 50 \\\n",
    "    --eval-steps 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\n",
      "saved_model.pb\n",
      "variables\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls output/keras_export/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wavesbucket-2\n",
      "Copying file://output/keras_export/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "/ [1 files][936.5 KiB/936.5 KiB]                                                \n",
      "Operation completed over 1 objects/936.5 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Copy your model file to Cloud Storage\n",
    "!echo $BUCKET_NAME\n",
    "!gsutil cp  output/keras_export/saved_model.pb  gs://$BUCKET_NAME\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ECN', 'ATT1', 'ATT2', 'ATT3', 'ATT4', 'ATT5', 'ATT6', 'ATT7', 'ATT8', 'ATT9', 'ATT10', 'ATT11', 'ATT12', 'ATT13', 'ATT14', 'ATT15', 'ATT16', 'ATT17', 'ATT18', 'ATT19', 'ATT20', 'ATT_CODE']\n",
      "[[0.88888884]\n",
      " [0.        ]\n",
      " [0.44444466]\n",
      " [0.7777777 ]\n",
      " [1.        ]]\n",
      "[[40025.   ]\n",
      " [32025.   ]\n",
      " [36025.004]\n",
      " [39025.   ]\n",
      " [41025.   ]]\n"
     ]
    }
   ],
   "source": [
    "from trainer import util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "'''\n",
    "_, _, eval_x, eval_y = util.load_data()\n",
    "'''\n",
    "\n",
    "#preprocess data\n",
    "eval_df = pd.read_csv('waves-validation-AI.csv', names=_CSV_COLUMNS, header=0, na_values='?')\n",
    "eval_df = util.preprocess(eval_df)\n",
    "\n",
    "#split features and labels\n",
    "eval_x, eval_y = eval_df, eval_df.pop('ATT_CODE')\n",
    "\n",
    "#standardise features\n",
    "eval_x = util.standardize(eval_x)\n",
    "\n",
    "#Take samples\n",
    "prediction_input = eval_x.sample(5)\n",
    "prediction_input=prediction_input.replace(np.nan,0)\n",
    "prediction_targets = np.array(eval_y[prediction_input.index])\n",
    "\n",
    "\n",
    "#standardise labels\n",
    "scaler = MinMaxScaler()\n",
    "prediction_targets=scaler.fit_transform(prediction_targets.reshape((-1,1)))\n",
    "print(prediction_targets)\n",
    "yy=scaler.inverse_transform(np.array(prediction_targets).reshape(-1,1))\n",
    "print(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ECN      ATT1      ATT2  ATT3  ATT4      ATT5  ATT6  ATT7      ATT8  \\\n",
      "8  0.693375 -1.752915  1.954340   0.0   0.0 -0.060432   0.0   0.0  0.403786   \n",
      "0 -1.525426  0.932402 -0.781736   0.0   0.0 -0.060432   0.0   0.0  0.403786   \n",
      "4 -0.416025  0.484849  0.000000   0.0   0.0 -0.785619   0.0   0.0  0.403786   \n",
      "7  0.416025 -1.305362  0.000000   0.0   0.0 -0.060432   0.0   0.0 -1.238733   \n",
      "9  0.970725  0.932402  0.000000   0.0   0.0 -1.510807   0.0   0.0  0.403786   \n",
      "\n",
      "       ATT9  ...  ATT18  ATT19  ATT20  workclass  marital_status  occupation  \\\n",
      "8  0.490854  ...    0.0    0.0    0.0         -1              -1          -1   \n",
      "0  0.490854  ...    0.0    0.0    0.0         -1              -1          -1   \n",
      "4 -1.760197  ...    0.0    0.0    0.0         -1              -1          -1   \n",
      "7 -1.760197  ...    0.0    0.0    0.0         -1              -1          -1   \n",
      "9  0.490854  ...    0.0    0.0    0.0         -1              -1          -1   \n",
      "\n",
      "   relationship  race  native_country  income_bracket  \n",
      "8            -1    -1              -1              -1  \n",
      "0            -1    -1              -1              -1  \n",
      "4            -1    -1              -1              -1  \n",
      "7            -1    -1              -1              -1  \n",
      "9            -1    -1              -1              -1  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "[[0.88888884]\n",
      " [0.        ]\n",
      " [0.44444466]\n",
      " [0.7777777 ]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction_input)\n",
    "print(prediction_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28,)\n",
      "(28,)\n",
      "(28,)\n",
      "(28,)\n",
      "(28,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "with open('test.json', 'w') as json_file:\n",
    "  for row in prediction_input.values.tolist():\n",
    "    x=np.array(row)\n",
    "    print (x.shape)\n",
    "    x=x.reshape(-1,1)  # no of timesteps/features\n",
    "    json.dump(x.tolist(), json_file)\n",
    "    json_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6933752298355103], [-1.7529150247573853], [1.9543399810791016], [0.0], [0.0], [-0.06043217331171036], [0.0], [0.0], [0.4037860929965973], [0.49085432291030884], [0.0], [-1.9277489185333252], [0.4021998345851898], [-0.28867536783218384], [0.6859626770019531], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]\n",
      "[[-1.5254255533218384], [0.9324015378952026], [-0.7817359566688538], [0.0], [0.0], [-0.06043217331171036], [0.0], [0.0], [0.4037860929965973], [0.49085432291030884], [0.0], [0.38554978370666504], [0.4021998345851898], [-0.28867536783218384], [0.6859626770019531], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]\n",
      "[[-0.41602516174316406], [0.4848487675189972], [0.0], [0.0], [0.0], [-0.7856193780899048], [0.0], [0.0], [0.4037860929965973], [-1.7601969242095947], [0.0], [0.38554978370666504], [0.4021998345851898], [-0.28867536783218384], [0.6859626770019531], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]\n",
      "[[0.41602516174316406], [-1.3053622245788574], [0.0], [0.0], [0.0], [-0.06043217331171036], [0.0], [0.0], [-1.238733172416687], [-1.7601969242095947], [0.0], [0.38554978370666504], [0.4021998345851898], [-0.28867536783218384], [0.6859626770019531], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]\n",
      "[[0.9707253575325012], [0.9324015378952026], [0.0], [0.0], [0.0], [-1.5108065605163574], [0.0], [0.0], [0.4037860929965973], [0.49085432291030884], [0.0], [0.38554978370666504], [-2.815398931503296], [-0.28867536783218384], [0.6859626770019531], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%bash\n",
    "\n",
    "cat test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENSE\n",
      "[0.8874058723449707]\n",
      "[0.2321588099002838]\n",
      "[0.9001837968826294]\n",
      "[0.22583416104316711]\n",
      "[0.9040400981903076]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If the signature defined in the model is not serving_default then you must specify it via --signature-name flag, otherwise the command may fail.\n",
      "WARNING: WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2021-01-04 02:49:50.202494: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200140000 Hz\n",
      "2021-01-04 02:49:50.203465: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ccb0290cd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-01-04 02:49:50.203505: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-01-04 02:49:50.203646: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:236: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:236: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:root:Error updating signature __saved_model_init_op: The name 'NoOp' refers to an Operation, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%bash\n",
    "\n",
    "gcloud ai-platform local predict \\\n",
    "    --model-dir output/keras_export/ \\\n",
    "    --json-instances ./test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected values\n",
      "[40025. 32025. 36025. 39025. 41025.]\n",
      "\n",
      "Predicted values\n",
      "[40011 34114 40126 34057 40161]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "#this are expected values\n",
    "print (\"Expected values\")\n",
    "scaler = MinMaxScaler()\n",
    "prediction_targets = np.array(eval_y[prediction_input.index])\n",
    "print (prediction_targets)\n",
    "#print (prediction_targets[0])\n",
    "prediction_targets=scaler.fit_transform(prediction_targets.reshape((-1,1)))\n",
    "\n",
    "\n",
    "#Predicted values\n",
    "print(\"\\nPredicted values\")\n",
    "#print(scaler.inverse_transform(np.array(0.8874058723449707).reshape(-1,1)))\n",
    "\n",
    "p=[]\n",
    "\n",
    "\n",
    "x=scaler.inverse_transform(np.array(0.8874058723449707).reshape(-1,1))\n",
    "p.append(x.astype('int64'))\n",
    "\n",
    "x=scaler.inverse_transform(np.array(0.2321588099002838).reshape(-1,1))\n",
    "p.append(x.astype('int64'))\n",
    "\n",
    "x=scaler.inverse_transform(np.array(0.9001837968826294).reshape(-1,1))\n",
    "p.append(x.astype('int64'))\n",
    "\n",
    "x=scaler.inverse_transform(np.array(0.22583416104316711).reshape(-1,1))\n",
    "p.append(x.astype('int64'))\n",
    "\n",
    "x=scaler.inverse_transform(np.array(0.9040400981903076).reshape(-1,1))\n",
    "p.append(x.astype('int64'))\n",
    "\n",
    "print(np.concatenate(p).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export TRAIN_DATA=gs://$BUCKET_NAME/wavesAI.csv\n",
    "export EVAL_DATA=gs://$BUCKET_NAME/waves-validation-AI.csv\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://test.json [Content-Type=application/json]...\n",
      "/ [1 files][  1.7 KiB/  1.7 KiB]                                                \n",
      "Operation completed over 1 objects/1.7 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil cp test.json gs://$BUCKET_NAME/data/test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "\n",
    "export TEST_JSON=gs://$BUCKET_NAME/data/test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: waves210104_033002\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [waves210104_033002] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe waves210104_033002\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs waves210104_033002\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "JOB_ID=waves$(date -u +%y%m%d_%H%M%S)\n",
    "OUTPUT_PATH=gs://$BUCKET_NAME/$JOB_ID\n",
    "gcloud ai-platform jobs submit training $JOB_ID \\\n",
    "    --job-dir $OUTPUT_PATH \\\n",
    "    --runtime-version $TFVERSION \\\n",
    "    --python-version $PYTHONVERSION \\\n",
    "    --module-name trainer.task \\\n",
    "    --package-path trainer/ \\\n",
    "    --region $REGION \\\n",
    "    -- \\\n",
    "    --train-files $TRAIN_DATA \\\n",
    "    --eval-files $EVAL_DATA \\\n",
    "    --train-steps 10 \\\n",
    "    --eval-steps 5 \\\n",
    "    --verbosity DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: os.environ[JOB_ID]: command not found\n",
      "/bin/bash: os.environ[MODEL_NAME]: command not found\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"JOB_ID\"] = \"waves210104_033002\" # Replace with your job id\n",
    "\n",
    "os.environ[\"MODEL_NAME\"] = \"wavesAIcentral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2021-01-04T03:30:04Z'\n",
      "endTime: '2021-01-04T03:38:52Z'\n",
      "etag: _65ajbilDt8=\n",
      "jobId: waves210104_033002\n",
      "startTime: '2021-01-04T03:37:52Z'\n",
      "state: SUCCEEDED\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --train-files\n",
      "  - --eval-files\n",
      "  - --train-steps\n",
      "  - '10'\n",
      "  - --eval-steps\n",
      "  - '5'\n",
      "  - --verbosity\n",
      "  - DEBUG\n",
      "  jobDir: gs://wavesbucket-2/waves210104_033002\n",
      "  packageUris:\n",
      "  - gs://wavesbucket-2/waves210104_033002/packages/45c58e1be43be11c68b5b815f445887e9990ee8dd40a2e93219b05b3d04664b0/trainer-0.0.0.tar.gz\n",
      "  pythonModule: trainer.task\n",
      "  pythonVersion: '3.7'\n",
      "  region: us-central1\n",
      "  runtimeVersion: '2.3'\n",
      "trainingOutput:\n",
      "  consumedMLUnits: 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/waves210104_033002?project=wavesproject\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Fwaves210104_033002&project=wavesproject\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai-platform jobs describe waves210104_033002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can create only in central region us-central1\n",
    "MODEL_NAME= \"wavesAIcentral\"\n",
    "os.environ[\"TFVERSION\"] = \"2.3\"\n",
    "os.environ[\"PYTHONVERSION\"] = \"3.7\"\n",
    "\n",
    "!gcloud ai-platform models create $MODEL_NAME --regions=$REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TFVERSION=\"2.3\"\n",
    "\n",
    "echo $TFVERSION\n",
    "echo $PYTHONVERSION\n",
    "MODEL_NAME=\"wavesAIcentral\"\n",
    "\n",
    "OUTPUT_PATH=gs://$BUCKET_NAME/$JOB_ID\n",
    "echo $MODEL_NAME\n",
    "\n",
    "MODEL_BINARIES=$OUTPUT_PATH/keras_export/\n",
    "echo $MODEL_BINARIES\n",
    "\n",
    "gcloud ai-platform versions create v1 \\\n",
    "--model $MODEL_NAME \\\n",
    "--origin $MODEL_BINARIES \\\n",
    "--runtime-version $TFVERSION \\\n",
    "--framework='TensorFlow'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            DEFAULT_VERSION_NAME\n",
      "ChurnPredictor  V1\n",
      "waves\n",
      "wavesAIcentral  v1\n",
      "wavescentral    v1\n",
      "wavescentral2   v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud ai-platform models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ai-platform predict \\\n",
    "--model $MODEL_NAME \\\n",
    "--version v1 \\\n",
    "--json-instances ./test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $MODEL_NAME\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "first_graph = tf.Graph()\n",
    "first_session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "with  first_session.as_default():\n",
    "    !gcloud ai-platform predict \\\n",
    "   --model $MODEL_NAME \\\n",
    "   --json-instances ./test.json \\\n",
    "   --version v1 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat ./test.json | head -1\n",
    "MODEL_NAME='wavesAIcentral'\n",
    "echo $MODEL_NAME\n",
    "\n",
    "gcloud ai-platform predict \\\n",
    "--model $MODEL_NAME \\\n",
    "--json-instances ./test.json \\\n",
    "--version v1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import util\n",
    "_, _, eval_x, eval_y = util.load_data()\n",
    "num_wit_examples = 5\n",
    "test_examples = np.hstack((eval_x[:num_wit_examples].values,eval_y[:num_wit_examples].reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import witwidget\n",
    "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
    "VERSION_NAME='v1'\n",
    "MODEL_NAME='wavesAIcentral'\n",
    "config_builder = (WitConfigBuilder(test_examples.tolist(), eval_x.columns.tolist() + ['ATT_CODE'])\n",
    "  .set_ai_platform_model(PROJECT, MODEL_NAME, VERSION_NAME)\n",
    "  .set_target_feature('ATT_CODE')\n",
    "  .set_label_vocab(['Attention Code']))\n",
    "WitWidget(config_builder, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
